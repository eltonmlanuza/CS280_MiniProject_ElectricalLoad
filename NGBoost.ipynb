{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost import NGBRegressor\n",
    "from scipy.stats import expon\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ngboost.scores import *\n",
    "from ngboost.distns import *\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def backup(object, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(object, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def reload(persist):\n",
    "    with open(persist, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# series = read_csv('ProcessedDataCenterHall.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "#series = read_csv('ProcessedBioEngineeringPV.csv', header=0, parse_dates=True)\n",
    "outputs_feature_select = reload(\"outputs_feature_select.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"ngboost_dataset.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_index(\"DateTime\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all features considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=7.9877 val_loss=8.0068 scale=1.0000 norm=40.8008\n",
      "[iter 100] loss=6.7058 val_loss=6.6950 scale=1.0000 norm=15.1599\n",
      "[iter 200] loss=5.7999 val_loss=5.8276 scale=2.0000 norm=12.0286\n",
      "[iter 300] loss=4.8410 val_loss=4.9867 scale=1.0000 norm=3.0493\n",
      "[iter 400] loss=4.2795 val_loss=4.5398 scale=1.0000 norm=2.7683\n",
      "[iter 500] loss=3.9501 val_loss=4.3400 scale=1.0000 norm=2.7749\n",
      "[iter 600] loss=3.6950 val_loss=4.2860 scale=0.5000 norm=1.3280\n",
      "[iter 700] loss=3.5697 val_loss=4.3004 scale=1.0000 norm=2.6962\n",
      "[iter 0] loss=7.9930 val_loss=8.0140 scale=1.0000 norm=40.7399\n",
      "[iter 100] loss=6.8185 val_loss=6.8992 scale=1.0000 norm=16.2044\n",
      "[iter 200] loss=6.0744 val_loss=6.2206 scale=1.0000 norm=7.4349\n",
      "[iter 300] loss=5.4826 val_loss=5.6883 scale=1.0000 norm=4.8381\n",
      "[iter 400] loss=4.9921 val_loss=5.3341 scale=1.0000 norm=4.0575\n",
      "[iter 500] loss=4.6948 val_loss=5.1582 scale=0.5000 norm=2.0181\n",
      "[iter 600] loss=4.4990 val_loss=5.1277 scale=1.0000 norm=3.9098\n",
      "[iter 700] loss=4.3554 val_loss=5.1378 scale=0.5000 norm=1.9665\n",
      "[iter 0] loss=7.9895 val_loss=8.0171 scale=1.0000 norm=40.7946\n",
      "[iter 100] loss=6.9159 val_loss=7.0566 scale=1.0000 norm=16.9513\n",
      "[iter 200] loss=6.2310 val_loss=6.4682 scale=1.0000 norm=8.3851\n",
      "[iter 300] loss=5.7339 val_loss=6.0284 scale=1.0000 norm=5.9683\n",
      "[iter 400] loss=5.2731 val_loss=5.7347 scale=1.0000 norm=4.9912\n",
      "[iter 500] loss=5.0126 val_loss=5.6144 scale=1.0000 norm=4.9244\n",
      "[iter 600] loss=4.8281 val_loss=5.6138 scale=0.5000 norm=2.4022\n",
      "[iter 700] loss=4.7179 val_loss=5.6491 scale=0.5000 norm=2.3842\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3485/2559823952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumlookahead\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumlookahead\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumlookahead\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mngbdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ngb_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbackup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngbdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ngbdict.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CS280/lib/python3.7/site-packages/ngboost/ngboost.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, X_val, Y_val, sample_weight, val_sample_weight, train_loss_monitor, val_loss_monitor, early_stopping_rounds)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         X, Y = check_X_y(\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         )\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CS280/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CS280/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n\u001b[0;32m--> 805\u001b[0;31m                         ensure_2d=False, dtype=None)\n\u001b[0m\u001b[1;32m    806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CS280/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/CS280/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "\n",
    "numlookahead = 4 # create numlookahead models\n",
    "ngbdict = {}\n",
    "for _ in range(1,numlookahead):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:-1*numlookahead*2+_*2], test_size=0.2, shuffle=False)\n",
    "\n",
    "    ngbdict[\"ngb_{}\".format(_)] = NGBRegressor(Dist=MultivariateNormal(k=2), Score=LogScore, minibatch_frac=0.2, n_estimators=800, verbose=True, random_state=2022).fit(X_train, Y_train, X_val=X_test, Y_val=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=7.9948 val_loss=8.0152 scale=1.0000 norm=40.9797\n",
      "[iter 100] loss=6.9994 val_loss=7.1560 scale=1.0000 norm=18.0572\n",
      "[iter 200] loss=6.3302 val_loss=6.6105 scale=1.0000 norm=9.0330\n",
      "[iter 300] loss=5.8356 val_loss=6.1741 scale=1.0000 norm=6.5607\n",
      "[iter 400] loss=5.3987 val_loss=5.8831 scale=1.0000 norm=5.4142\n",
      "[iter 500] loss=5.1396 val_loss=5.7743 scale=1.0000 norm=5.3494\n",
      "[iter 600] loss=4.9615 val_loss=5.7811 scale=0.5000 norm=2.5935\n",
      "[iter 700] loss=4.8810 val_loss=5.8239 scale=0.5000 norm=2.5912\n"
     ]
    }
   ],
   "source": [
    "#last one (due to indexing issues)\n",
    "_ = numlookahead\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:], test_size=0.2, shuffle=False)\n",
    "\n",
    "ngbdict[\"ngb_{}\".format(_)] = NGBRegressor(Dist=MultivariateNormal(k=2), Score=LogScore, minibatch_frac=0.2, n_estimators=800, verbose=True, random_state=2022).fit(X_train, Y_train, X_val=X_test, Y_val=Y_test)\n",
    "\n",
    "#save the file\n",
    "backup(ngbdict,\"ngbdict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['realpowerin_15min_0',\n",
       " 'reactivepowerin_15min_0',\n",
       " 'reactivepowerin_1Hr_22',\n",
       " 'realpowerin_1Hr_21',\n",
       " 'realpowerin_1D_0',\n",
       " 'reactivepowerin_1Hr_21',\n",
       " 'realpowerin_1Hr_11',\n",
       " 'realpowerin_1Hr_13',\n",
       " 'reactivepowerin_1Hr_23',\n",
       " 'realpowerin_1Hr_7',\n",
       " 'reactivepowerin_1D_0',\n",
       " 'realpowerin_1Hr_15',\n",
       " 'realpowerin_1Hr_9',\n",
       " 'realpowerin_1Hr_23',\n",
       " 'realpowerin_15min_4',\n",
       " 'realpowerin_1Hr_19',\n",
       " 'reactivepowerin_1Hr_20',\n",
       " 'reactivepowerin_15min_3',\n",
       " 'realpowerin_1Hr_17',\n",
       " 'realpowerin_15min_12',\n",
       " 'reactivepowerin_15min_6',\n",
       " 'realpowerin_1Hr_2']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_feature_select[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetreduced=pd.concat([dataset[outputs_feature_select[0]],dataset.iloc[:,-1*numlookahead*2:]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=7.9877 val_loss=8.0109 scale=1.0000 norm=40.8008\n",
      "[iter 100] loss=6.7509 val_loss=6.7899 scale=1.0000 norm=15.6061\n",
      "[iter 200] loss=5.8871 val_loss=5.9890 scale=1.0000 norm=6.4282\n",
      "[iter 300] loss=5.1354 val_loss=5.3346 scale=1.0000 norm=3.8271\n",
      "[iter 400] loss=4.6521 val_loss=4.9595 scale=1.0000 norm=3.5031\n",
      "[iter 500] loss=4.3399 val_loss=4.7883 scale=1.0000 norm=3.4173\n",
      "[iter 600] loss=4.1282 val_loss=4.7388 scale=1.0000 norm=3.3781\n",
      "[iter 700] loss=4.0190 val_loss=4.7368 scale=1.0000 norm=3.3605\n",
      "[iter 0] loss=7.9930 val_loss=8.0099 scale=1.0000 norm=40.7399\n",
      "[iter 100] loss=6.8815 val_loss=7.0347 scale=1.0000 norm=16.6698\n",
      "[iter 200] loss=6.1975 val_loss=6.4066 scale=1.0000 norm=8.0120\n",
      "[iter 300] loss=5.6193 val_loss=5.9201 scale=1.0000 norm=5.4241\n",
      "[iter 400] loss=5.1942 val_loss=5.6204 scale=1.0000 norm=4.7859\n",
      "[iter 500] loss=4.9363 val_loss=5.4851 scale=1.0000 norm=4.7708\n",
      "[iter 600] loss=4.7641 val_loss=5.4542 scale=1.0000 norm=4.6982\n",
      "[iter 700] loss=4.6679 val_loss=5.4509 scale=1.0000 norm=4.6718\n",
      "[iter 0] loss=7.9895 val_loss=8.0141 scale=1.0000 norm=40.7946\n",
      "[iter 100] loss=6.9701 val_loss=7.1387 scale=1.0000 norm=17.3136\n",
      "[iter 200] loss=6.3085 val_loss=6.5739 scale=1.0000 norm=8.7587\n",
      "[iter 300] loss=5.8167 val_loss=6.1306 scale=1.0000 norm=6.3351\n",
      "[iter 400] loss=5.3973 val_loss=5.8626 scale=2.0000 norm=10.9610\n",
      "[iter 500] loss=5.1611 val_loss=5.7557 scale=1.0000 norm=5.4406\n",
      "[iter 600] loss=5.0053 val_loss=5.7384 scale=0.5000 norm=2.6850\n",
      "[iter 700] loss=4.9392 val_loss=5.7457 scale=1.0000 norm=5.3039\n"
     ]
    }
   ],
   "source": [
    "ngbdict = {}\n",
    "for _ in range(1,numlookahead):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(datasetreduced.iloc[:,:-1*numlookahead*2], datasetreduced.iloc[:,-1*numlookahead*2+(_-1)*2:-1*numlookahead*2+_*2], test_size=0.2, shuffle=False)\n",
    "\n",
    "    ngbdict[\"ngb_{}\".format(_)] = NGBRegressor(Dist=MultivariateNormal(k=2), Score=LogScore, minibatch_frac=0.2, n_estimators=800, verbose=True, random_state=2022).fit(X_train, Y_train, X_val=X_test, Y_val=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=7.9948 val_loss=8.0252 scale=0.5000 norm=20.4899\n",
      "[iter 100] loss=7.0170 val_loss=7.1741 scale=1.0000 norm=17.9699\n",
      "[iter 200] loss=6.3641 val_loss=6.6380 scale=1.0000 norm=9.1635\n",
      "[iter 300] loss=5.8885 val_loss=6.2201 scale=1.0000 norm=6.7913\n",
      "[iter 400] loss=5.4823 val_loss=5.9553 scale=1.0000 norm=5.7286\n",
      "[iter 500] loss=5.2310 val_loss=5.8595 scale=1.0000 norm=5.6279\n",
      "[iter 600] loss=5.0885 val_loss=5.8493 scale=0.5000 norm=2.7707\n",
      "[iter 700] loss=5.0197 val_loss=5.8565 scale=0.5000 norm=2.7832\n"
     ]
    }
   ],
   "source": [
    "_ = numlookahead\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(datasetreduced.iloc[:,:-1*numlookahead*2], datasetreduced.iloc[:,-1*numlookahead*2+(_-1)*2:], test_size=0.2, shuffle=False)\n",
    "\n",
    "ngbdict[\"ngb_{}\".format(_)] = NGBRegressor(Dist=MultivariateNormal(k=2), Score=LogScore, minibatch_frac=0.2, n_estimators=800, verbose=True, random_state=2022).fit(X_train, Y_train, X_val=X_test, Y_val=Y_test)\n",
    "\n",
    "#save the file\n",
    "backup(ngbdict,\"ngbdictreduced.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all features below\n",
    "\n",
    "[iter 0] loss=7.9948 val_loss=8.0152 scale=1.0000 norm=40.9797 \\\n",
    "[iter 100] loss=6.9994 val_loss=7.1560 scale=1.0000 norm=18.0572 \\\n",
    "[iter 200] loss=6.3302 val_loss=6.6105 scale=1.0000 norm=9.0330 \\\n",
    "[iter 300] loss=5.8356 val_loss=6.1741 scale=1.0000 norm=6.5607 \\\n",
    "[iter 400] loss=5.3987 val_loss=5.8831 scale=1.0000 norm=5.4142 \\\n",
    "[iter 500] loss=5.1396 val_loss=5.7743 scale=1.0000 norm=5.3494 \\\n",
    "[iter 600] loss=4.9615 val_loss=5.7811 scale=0.5000 norm=2.5935 \\\n",
    "[iter 700] loss=4.8810 val_loss=5.8239 scale=0.5000 norm=2.5912\n",
    "\n",
    "### reduced features below\n",
    "\n",
    "[iter 0] loss=7.9948 val_loss=8.0252 scale=0.5000 norm=20.4899 \\\n",
    "[iter 100] loss=7.0170 val_loss=7.1741 scale=1.0000 norm=17.9699 \\\n",
    "[iter 200] loss=6.3641 val_loss=6.6380 scale=1.0000 norm=9.1635 \\\n",
    "[iter 300] loss=5.8885 val_loss=6.2201 scale=1.0000 norm=6.7913 \\\n",
    "[iter 400] loss=5.4823 val_loss=5.9553 scale=1.0000 norm=5.7286 \\\n",
    "[iter 500] loss=5.2310 val_loss=5.8595 scale=1.0000 norm=5.6279 \\\n",
    "[iter 600] loss=5.0885 val_loss=5.8493 scale=0.5000 norm=2.7707 \\\n",
    "[iter 700] loss=5.0197 val_loss=5.8565 scale=0.5000 norm=2.7832\n",
    "### from the runs, by inspection, it appears that the validation loss is lower when all features are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deterministic Values\n",
    "# print(\"Y_preds: \", Y_preds)\n",
    "# print(\"Y_dist: \", Y_dists)\n",
    "\n",
    "# # Probabilistic Values\n",
    "# mean = Y_dists.mean\n",
    "# sample = Y_dists.rv()\n",
    "# scipy_list = Y_dists.scipy_distribution()\n",
    "\n",
    "# print(\"mean: \", mean)\n",
    "# print(\"sample: \", sample)\n",
    "\n",
    "# # test Mean Squared Error\n",
    "# test_MSE = mean_squared_error(Y_preds, Y_test)\n",
    "# print('Test MSE', test_MSE)\n",
    "\n",
    "# # test Negative Log Likelihood\n",
    "# test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "# print('Test NLL', test_NLL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of vectorized errors\n",
    "ngbdict = reload(\"ngbdict.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_norms = []\n",
    "for _ in range(1,numlookahead):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:-1*numlookahead*2+_*2], test_size=0.2, shuffle=False)\n",
    "    Y_preds = ngbdict['ngb_{}'.format(_)].predict(X_test)\n",
    "    combine_norms.append(np.mean(np.apply_along_axis(LA.norm,1,Y_preds-Y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = 4\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:], test_size=0.2, shuffle=False)\n",
    "Y_preds = ngbdict['ngb_{}'.format(_)].predict(X_test)\n",
    "combine_norms.append(np.mean(np.apply_along_axis(LA.norm,1,Y_preds-Y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.91896561750159"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(combine_norms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0266962339931736, 4.556193487748652, 5.801472591165884, 6.291500157098652]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean mean loss for point prediction 4.91896561750159 with point model losses of [3.0266962339931736, 4.556193487748652, 5.801472591165884, 6.291500157098652]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start of Region Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _=1\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:-1*numlookahead*2+_*2], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NLL 4.335625794645859\n"
     ]
    }
   ],
   "source": [
    "# test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "# print('Test NLL', test_NLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_dists = ngbdict['ngb_{}'.format(1)].pred_dist(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "boundaryval = chi2.ppf(0.25,Y_test.shape[1]) # quartile function\n",
    "\n",
    "def det_pointbelong(cov,yval,mean):\n",
    "    \"\"\"\n",
    "    :param cov: covariance matrix\n",
    "    :param yval: test values\n",
    "    :param mean: mean value of the model at that point\n",
    "    returns: Boolean value whether \n",
    "    \"\"\"\n",
    "    return np.matmul(np.matmul(yval-mean,np.linalg.inv(cov)),np.transpose(yval-mean)) <= boundaryval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov= Y_dists.params['scale'][0]\n",
    "# yval = Y_test.iloc[0].values\n",
    "# mean = Y_dists.params['loc'][0]\n",
    "\n",
    "combine_percent_belong = []\n",
    "for _ in range(1,numlookahead): # 1-3 pa lang yan\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:-1*numlookahead*2+_*2], test_size=0.2, shuffle=False)\n",
    "    Y_dists = ngbdict['ngb_{}'.format(_)].pred_dist(X_test)\n",
    "    accum_belong = []\n",
    "\n",
    "    for ct in range(Y_dists.params['loc'].shape[0]):\n",
    "        accum_belong.append(det_pointbelong(Y_dists.params['scale'][ct],Y_test.iloc[ct,:].values,Y_dists.params['loc'][ct]))\n",
    "    \n",
    "    dfbools = pd.DataFrame(np.unique(np.array(accum_belong),return_counts=True))\n",
    "    dfbools.columns = dfbools.iloc[0,:]\n",
    "    combine_percent_belong.append(dfbools.loc[1,True]/(dfbools.loc[1,True]+dfbools.loc[1,False]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=4\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:,:-1*numlookahead*2], dataset.iloc[:,-1*numlookahead*2+(_-1)*2:], test_size=0.2, shuffle=False)\n",
    "Y_dists = ngbdict['ngb_{}'.format(_)].pred_dist(X_test)\n",
    "accum_belong = []\n",
    "\n",
    "for ct in range(Y_dists.params['loc'].shape[0]):\n",
    "    accum_belong.append(det_pointbelong(Y_dists.params['scale'][ct],Y_test.iloc[ct,:].values,Y_dists.params['loc'][ct]))\n",
    "\n",
    "dfbools = pd.DataFrame(np.unique(np.array(accum_belong),return_counts=True))\n",
    "dfbools.columns = dfbools.iloc[0,:]\n",
    "combine_percent_belong.append(dfbools.loc[1,True]/(dfbools.loc[1,True]+dfbools.loc[1,False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.255526414387411,\n",
       " 0.25814911952041963,\n",
       " 0.22068190333458224,\n",
       " 0.21693518171599852]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_percent_belong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the fraction of datapoints belonging in the quartile region\n",
    "[0.255526414387411,\n",
    " 0.25814911952041963,\n",
    " 0.22068190333458224,\n",
    " 0.21693518171599852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a93b84ea9893d212d92b68e0592d8422e6dcb71276c756797a9534fa665d1f08"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
